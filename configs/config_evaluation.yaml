save_evaluation: true
save_detailed_results: false # Whether to save detailed results for each file or just the overview

test_name: "v3_GroundTruthSynthetic_llama3" # Name of the test run to evaluate (will use latest run with this name)
ground_truth_dir: "data/ground_truth/ground_truth_synthetic" # Directory containing ground truth files
output_dir: "runs/evaluations" # Directory to save evaluation results

# Evaluation settings
entity_similarity_threshold: 80 # Threshold for entity matching (0-100)
relationship_similarity_threshold: 80 # Threshold for relationship matching (0-100)
