save_evaluation: true
save_detailed_results: false # Whether to save detailed results for each file or just the overview

test_name: "v4_GroundTruthReal_openaiGPT4oMini" # Name of the test run to evaluate (will use latest run with this name)
ground_truth_dir: "data/ground_truth/ground_truth_real_AGENT" # Directory containing ground truth files
output_dir: "runs/evaluations/latest" # Directory to save evaluation results

# Evaluation settings
entity_similarity_threshold: 80 # Threshold for entity matching (0-100)
relationship_similarity_threshold: 80 # Threshold for relationship matching (0-100)
