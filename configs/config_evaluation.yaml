save_evaluation: true

test_name: "v3_subsample_groundTruth_openai" # Name of the test run to evaluate (will use latest run with this name)
ground_truth_dir: "data/ground_truth" # Directory containing ground truth files
ground_truth_subdir: "LLM_revised_100news_promptv3" # Optional subdirectory within ground_truth_dir to use
output_dir: "runs/evaluations" # Directory to save evaluation results

# Evaluation settings
entity_similarity_threshold: 80 # Threshold for entity matching (0-100)
relationship_similarity_threshold: 80 # Threshold for relationship matching (0-100)
